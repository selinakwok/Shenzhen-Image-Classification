{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOG0027  Google Earth Engine (GEE) for Shenzhen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical will use Google Earth Engine (GEE)'s python library [EE](https://developers.google.com/earth-engine) and [geemap library](https://geemap.org/). We will be applying the techniques and setup used previously in the course to the Shenzhen region. Make sure the prior lesson [introducing GEE](https://github.com/UCL-EO/GEOG0027/blob/main/docs/Intro_to_GEE.ipynb) has been completed.\n",
    "\n",
    "After the first GEE practical lesson you will be able to use UCL's [Jupyter Hub](https://jupyter.data-science.rc.ucl.ac.uk/), and from there open a `Terminal` and use the following command to get the required files. Then open this notebook in jupyterhub to continue.\n",
    "\n",
    "`wget https://github.com/UCL-EO/GEOG0027_Coursework/raw/main/docs/3_PythonGEE-Shenzhen.ipynb`\n",
    "\n",
    "\n",
    "## First map\n",
    "For the Shenzhen classification work, we start from displaying a basemap of the area. The first time when you use GEE, you will need to have a Google account and provide an authorization code as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_U0PyVMPOXcFyn33JOV7k6xpA79V8lt9lMQLpzUffLs&tc=z2ucxluV2QjcoWI0l3BOk-_75MtpIa50LD3OF4rOqAQ&cc=VIzKfJtHLV-O0q8B9gkjQJbbNZMuyqTWsjxqGupak9c>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=_U0PyVMPOXcFyn33JOV7k6xpA79V8lt9lMQLpzUffLs&tc=z2ucxluV2QjcoWI0l3BOk-_75MtpIa50LD3OF4rOqAQ&cc=VIzKfJtHLV-O0q8B9gkjQJbbNZMuyqTWsjxqGupak9c</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AZEOvhUmDpyc0RBCSjl6dylh27t43_Eq0mB3eavpi4wWtMetfvAKNOn9YpI\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a13341c60446ca8b5a0f87ceefb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[22.634, 114.19], controls=(WidgetControl(options=['position', 'transparent_bg'], widget=HBox(child…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import geemap, ee, os, numpy\n",
    "import ipyleaflet\n",
    "\n",
    "Map = geemap.Map(center=[22.634, 114.19], zoom=9)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic Google Map-like interface should be displayed here now. If you can't see anything, please ensure that the ipyleaflet nbextension has been enabled, and the appropriate Kernel has been selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the time series\n",
    "Let's define a rectangular region of interest, following [min lon, min lat, max mon, max lat] first, and display a short 'movie' (a .gif file in fact) of how this area has changed over the past decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shenzhen_rec = ee.Geometry.Rectangle([113.7659, 22.40, 114.6654, 22.8536]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map_gif = geemap.Map(center=[22.7511, 113.91], zoom=10)\n",
    "Map_gif.add_landsat_ts_gif(roi=shenzhen_rec, start_year=1985, bands=['NIR', 'Red', 'Green'], frames_per_second=5)\n",
    "Map_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output map 'Map_gif' should look like something below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![shenzhengif](images/landsat_ts_sgn.gif \"shenzhen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compare such change by using a slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba4586699d04849a7da0bf8f10a6233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[22.627302103068118, 114.2156500000001], controls=(WidgetControl(options=['position', 'transparent_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "landsat_ts = geemap.landsat_timeseries(roi=shenzhen_rec, start_year=1986, end_year=2020, \\\n",
    "                                       start_date='01-01', end_date='12-31')\n",
    "\n",
    "layer_names = ['Landsat ' + str(year) for year in range(1986, 2021)]\n",
    "\n",
    "geemap_landsat_vis = {\n",
    "    'min': 0,\n",
    "    'max': 3000,\n",
    "    'gamma': [1, 1, 1],\n",
    "    'bands': ['NIR', 'Red', 'Green']} # You can change the vis bands here\n",
    "\n",
    "Map2 = geemap.Map()\n",
    "Map2.ts_inspector(left_ts=landsat_ts, right_ts=landsat_ts, left_names=layer_names, right_names=layer_names, \\\n",
    "                 left_vis=geemap_landsat_vis, right_vis=geemap_landsat_vis)\n",
    "Map2.centerObject(shenzhen_rec, zoom=10)\n",
    "Map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have previously defined a rectangle for Shenzhen area by coordinates, but we can also use existing shape/vector files to select Areas of Interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select 'Shenzhen' as the area of interest (AOI)\n",
    "The vector border layer is imported from https://developers.google.com/earth-engine/datasets/tags/borders, which includes the [Global Administrative Unit Layers (GAUL) data](https://developers.google.com/earth-engine/datasets/catalog/FAO_GAUL_2015_level2) from 2015. You may notice that Shenzhen's boundary has expanded since (e.g. coastal landfill). We can, for example, manually draw/define another polygon and clip it to the GAUL border file, or, as a simple example, we will add a 'buffer' (e.g. 3000 meters) to the GAUL boundary data. This inevitably will introduce some areas outside the border of Shenzhen, e.g. part of Hong Kong, so you can work out some more elegant way of combining/clipping multiple AOI layers if time allows. Please justify your choice and summarize how you made it in the report. In the following example, I will use the `shenzhen_buffer` as my AOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a13341c60446ca8b5a0f87ceefb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(bottom=7287.0, center=[25.589207384633696, 110.92804657784917], controls=(WidgetControl(options=['position…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cities = ee.FeatureCollection(\"FAO/GAUL/2015/level2\")\n",
    "#Map.addLayer(cities, {}, 'Cities', False)\n",
    "\n",
    "shenzhen = cities.filter(ee.Filter.eq('ADM2_NAME', 'Shenzhen'))\n",
    "outline = ee.Image().byte().paint(**{\n",
    "  'featureCollection': shenzhen,\n",
    "  'color': 1,\n",
    "  'width': 3\n",
    "})\n",
    "Map.addLayer(outline, {}, 'Shenzhen')\n",
    "\n",
    "# Next, add some buffer to include the coastal expansion areas\n",
    "shenzhen_buffer = ee.Geometry(shenzhen.geometry()).buffer(5000)\n",
    "Map.addLayer(shenzhen_buffer, {}, 'Buffer around Shenzhen')\n",
    "#Map.addLayer(rec, \"Original rec bounds\")\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export shenzhen buffer\n",
    "shenzhen_export = ee.FeatureCollection(shenzhen_buffer)\n",
    "\n",
    "task = ee.batch.Export.table.toDrive(collection=shenzhen_export,\n",
    "                                     description='shenzhen mask',\n",
    "                                     fileFormat='SHP')\n",
    "\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Landsat data collections from GEE\n",
    "Now we can see the the buffered AOI displayed on the Map. Next, let's load some Landsat images for the Shenzhen area. I've defined here a python function called `display_landsat_collection` to do so. It automatically loads both the [surface reflectance](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR) and [annual NDVI](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_ANNUAL_NDVI) image collections from GEE's data catalog and also calculates the annual means for each band. \n",
    "\n",
    "You can skip most of the details of what's inside the code cell, but only to look at the first (and last) line of code. In order to run such function, you will need to choose a year (any year since 1984) and an AOI. In the following example, I choose year 2019 and the Shenzhen buffer to demonstrate the use of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_vis_param = {\n",
    "            'min': 0,\n",
    "            'max': 3000,\n",
    "            'bands': ['NIR', 'Red', 'Green']  # False Colour Composit bands to be visualised \n",
    "}\n",
    "ndvi_colorized_vis = {\n",
    "            'min': 0.0,\n",
    "            'max': 1.0,\n",
    "            'palette': [\n",
    "            'FFFFFF', 'CE7E45', 'DF923D', 'F1B555', 'FCD163', '99B718', '74A901',\n",
    "            '66A000', '529400', '3E8601', '207401', '056201', '004C00', '023B01',\n",
    "            '012E01', '011D01', '011301']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_landsat_collection(Map,year, aoi, cloud_tolerance = 3.0, \n",
    "                            DISPLAY_ON_MAP = False, MEDIAN_ONLY = False):\n",
    "    '''This function allows GEE to display a Landsat data collection \n",
    "    from any year between 1984 and present year\n",
    "    that fall within the AOI and cloud tolerance, e.g. 3.0%.\n",
    "    There are two optional flag:\n",
    "    When DISPLAY_ON_MAP is TRUE, display this layer onto Map;\n",
    "    When return_series = 'MEDIAN_ONLY', only median SR is loaded into landsat_ts, and\n",
    "    Setting this option to MEDIAN_ONLY would be faster than loading other collections. \n",
    "    '''\n",
    "    assert year >= 1984\n",
    "    \n",
    "    def renameBandsETM(image):\n",
    "        if year >=2013: #LS8\n",
    "            bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7'] #, 'pixel_qa'\n",
    "            new_bands = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2'] #, 'pixel_qa'\n",
    "#             return image.select(bands).rename(new_bands)\n",
    "        elif year >=1984:\n",
    "            bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B7',]# 'pixel_qa']\n",
    "            new_bands = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2',]# 'pixel_qa']\n",
    "#         else: \n",
    "#             [B1, B2, B3, B4, B5, B6, B7]\n",
    "        return image.select(bands).rename(new_bands)\n",
    "        \n",
    "    if not(MEDIAN_ONLY):\n",
    "        if year >= 2013:\n",
    "            layer_name = 'LC08' # LS8: 2013-now        \n",
    "        elif year == 2012: # # LS7: 1999- , however SLC error >= 1999:\n",
    "            layer_name = 'LE07' \n",
    "        elif year >=1984:\n",
    "            layer_name = 'LT05' # LS5: 1984-2012\n",
    "       \n",
    "        collection_name_sr = f\"LANDSAT/{layer_name}/C01/T1_SR\" \n",
    "        # You can also use the following line, if interested in incorperating ndvi\n",
    "        collection_name_ndvi = f\"LANDSAT/{layer_name}/C01/T1_ANNUAL_NDVI\" \n",
    "\n",
    "        all_sr_image = ee.ImageCollection(collection_name_sr) \\\n",
    "            .filterBounds(aoi) \\\n",
    "            .filterDate(f'{year}-01-01', f'{year}-12-31') \\\n",
    "            .filter(ee.Filter.lt('CLOUD_COVER', cloud_tolerance))\\\n",
    "            .sort('system:time_start') \\\n",
    "            .select('B[1-7]') \\\n",
    "            .sort('CLOUD_COVER')\n",
    "\n",
    "        all_sr_image = all_sr_image.map(renameBandsETM) # rename bands with 'renameBandsETM' function\n",
    "        \n",
    "        # reduce all_sr_image to annual average per pixel\n",
    "        mean_image = all_sr_image.mean()\n",
    "        mean_image = mean_image.clip(aoi).unmask()\n",
    "\n",
    "        ndvi_image = ee.ImageCollection(collection_name_ndvi)\\\n",
    "            .filterBounds(aoi) \\\n",
    "            .filterDate(f'{year}-01-01', f'{year}-12-31')\\\n",
    "            .select('NDVI')\\\n",
    "            .first()\n",
    "        ndvi_image = ndvi_image.clip(aoi).unmask()\n",
    "\n",
    "        #mean_image.addBands(ndvi_image, 'NDVI')\n",
    "    \n",
    "    # This line loads all annual median surface ref - one image per year\n",
    "    landsat_ts = geemap.landsat_timeseries(roi=aoi, start_year=year, end_year=year, \\\n",
    "                                       start_date='01-01', end_date='12-31')\n",
    "\n",
    "    median_image = landsat_ts.first().clip(aoi).unmask() # .first() because only one year, first (only) image is the year\n",
    "    \n",
    "    if type(DISPLAY_ON_MAP) == geemap.geemap.Map:\n",
    "        \n",
    "        \"\"\"if not(MEDIAN_ONLY):\n",
    "            Map.addLayer(ndvi_image, ndvi_colorized_vis, 'NDVI '+str(year),  opacity=0.9)\n",
    "            Map.addLayer(mean_image, landsat_vis_param, \"Mean Ref \"+str(year))\n",
    "        Map.addLayer(median_image, landsat_vis_param, \"Median Ref \"+str(year))\"\"\"\n",
    "        \n",
    "        if not(MEDIAN_ONLY):\n",
    "            Map.addLayer(ndvi_image, ndvi_colorized_vis, 'NDVI ',  opacity=0.9)\n",
    "            Map.addLayer(mean_image, landsat_vis_param, \"Mean Ref \")\n",
    "        Map.addLayer(median_image, landsat_vis_param, \"Median Ref \")\n",
    "\n",
    "    if MEDIAN_ONLY:\n",
    "        return median_image\n",
    "    else:\n",
    "        return all_sr_image, mean_image, median_image, ndvi_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `load_landsat_collection` function has been defined, and we will run/execute it by calling the function name with appropriate input parameters (or 'arguments). The output of such function will be returned to the variables on the LHS of the equal sign, i.e. all_image_2019, mean_2019, median_2019, and ndvi_2019 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8a13341c60446ca8b5a0f87ceefb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(bottom=114433.0, center=[22.64823470853169, 114.2351531982422], controls=(WidgetControl(options=['position…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map = geemap.Map(center=[22.634, 114.19], zoom=10)\n",
    "\n",
    "# All you need to modify here is the YEAR below:\n",
    "year = 2012\n",
    "all_image, mean, median, ndvi = load_landsat_collection(Map,year,\\\n",
    "                                        shenzhen_buffer, cloud_tolerance = 3,\\\n",
    "                                        DISPLAY_ON_MAP = Map)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should examine the functions under the `toolbar` and `layer` buttons on the top-right corner of the Map, e.g. use the `inspector` and `plotting` tools to check the data values, or use `layers` control to switch on/off layers or to adjust opacity.\n",
    "\n",
    "We can also check the metadata from the Landsat image collection we just loaded from the Cloud. Have a look of the output. Any useful information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images used for mean composite\n",
    "first_image = all_image.first() \n",
    "￼\n",
    "props = geemap.image_props(first_image)\n",
    "print( props.getInfo())\n",
    "#print(first_image.get('IMAGE_DATE').getInfo())\n",
    "#print(first_image.get('CLOUD_COVER').getInfo(), '%')\n",
    "\n",
    "# get list of individual images \n",
    "image_list = all_image.aggregate_array('system:id')\n",
    "image_list.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "landsat_vis_param_single = {\n",
    "            'min': 0,\n",
    "            'max': 3000,\n",
    "            'bands': ['B4', 'B3', 'B2']  # False Colour Composit bands to be visualised \n",
    "}\n",
    "\n",
    "image_name = 'LANDSAT/LE07/C01/T1_SR/LE07_121044_20001110',\n",
    "single_image = ee.Image(image_name)\n",
    "single_image.bandNames().getInfo()\n",
    "Map.addLayer(single_image, landsat_vis_param_single, \"single image\")\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.landsat_timeseries.__doc__\n",
    "geemap.landsat_timeseries??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveband info\n",
    "first_image = all_image.first() \n",
    "print(first_image.bandNames().getInfo())\n",
    "print(median.bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine the mean, median surface reflectance and/or NDVI layers we've visualized. Which one is better? Which band should we include into the classification?\n",
    "\n",
    "We can also use the Shenzhen GEE app from the [previous chapter](4_DownloadLandsatFromGEE.ipynb) to examine the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import IFrame\n",
    "IFrame('https://plewis.users.earthengine.app/view/shenzhen','100%',490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to switching layers on and off, adjusting opacity, we can also use python code to some simple mathmatical operations, e.g. calculating the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_ndvi = mean.normalizedDifference(['NIR', 'Red'])\n",
    "median_ndvi = median.normalizedDifference(['NIR', 'Red'])\n",
    "median_ndwi = median.normalizedDifference(['Green','NIR'])\n",
    "\n",
    "Map.addLayer(median_ndvi ,ndvi_colorized_vis, 'Median NDVI')\n",
    "Map.addLayer(mean_ndvi.subtract(median_ndvi), {'min': -0.2,'max': 0.2}, 'Diff in NDVI')\n",
    "Map.addLayer(median_ndwi, ndvi_colorized_vis, 'NDWI from Median LS')\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means unsupervised classification with GEE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the 2019 Median image as an example. In the following code cell, a function named `unsupervised_classifier` has been defined to classify an image by take five input parameters (or 'arguments'). You can don't have to worry about the detail of the code except the very first line, where information about the 'arguments' can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def unsupervised_classifier(image, aoi, year, n_clusters=3, output_filename='', DISPLAY_ON_MAP = False):\n",
    "    '''This function provides a simple K-means classifier,\n",
    "    with a default no. of cluster of 5. User will need to specify \n",
    "    an AOI and an image to be classified.\n",
    "    Optional arguments:\n",
    "    n_clusters defines the number of clusters in the K-means classifier;\n",
    "    output_filename should be a quoted string, e.g. 'Shenzhen_Landsat_Kmeans_2019.tif';\n",
    "    DISPLAY_ON_MAP can be switched on, so the cluster map will be added to Map.\n",
    "    '''\n",
    "    \n",
    "    # Make the training dataset:\n",
    "    training_points = image.sample(**{\n",
    "        'region': aoi,\n",
    "        'scale': 30,\n",
    "        'numPixels': 5000,\n",
    "        'seed': 0,\n",
    "        'geometries': True  # Set this to False to ignore geometries\n",
    "    })\n",
    "    Rcolors = ['#%02x%02x%02x' % tuple((int(c*254) for c in plt.cm.plasma(n)[:-1])) \n",
    "           for n in range(0,255,int(255/n_clusters))]\n",
    "\n",
    "    #Map.addLayer(training_points, {}, 'training points', False) # No need to visualise this layer\n",
    "\n",
    "    # Instantiate the clusterer and train it.\n",
    "    clusterer = ee.Clusterer.wekaKMeans(n_clusters).train(training_points)\n",
    "\n",
    "    # Cluster the input using the trained clusterer.\n",
    "    class_result = image.cluster(clusterer)\n",
    "    \n",
    "    # Reclassify the map to avoid zero, in case of masking. E.g. from [0, 1, 2, 3, 4] to [1, 2, 3, 4, 5]\n",
    "    #class_result = class_result.remap(list(range(0, n_clusters)), list(range(1, n_clusters+1)))\n",
    "\n",
    "    if type(DISPLAY_ON_MAP) == geemap.geemap.Map:\n",
    "        # Display the clusters with random colors.\n",
    "        Map.addLayer(class_result, {'min': 0, 'max': n_clusters-1,'palette':Rcolors}, 'Kmeans '+str(n_clusters)+' Clusters '+str(year), opacity=1.0)\n",
    "        legend_keys = ['Cluster {}'.format(n) for n in range(n_clusters)]\n",
    "        Map.add_legend(legend_keys=legend_keys, legend_colors=Rcolors, position='bottomright')\n",
    "\n",
    "    if output_filename == '':\n",
    "        print(f'{year} classification finished. No output exported.')\n",
    "    else:\n",
    "        #Export the result directly to your computer/Hub:\n",
    "        geemap.ee_export_image(class_result, filename=output_filename, \\\n",
    "                            scale=900, region=aoi, file_per_band=False) \n",
    "        # When scale is small, GEE won't allow downloading due to size limitation\n",
    "\n",
    "    return class_result\n",
    "\n",
    "def calculate_class_size(class_result, year, legend_keys, PRINT_STATS=False):\n",
    "    n_clusters = len(legend_keys)\n",
    "    \n",
    "    #landsat_stats = geemap.image_stats(class_result, scale=90)\n",
    "    #print(landsat_stats.getInfo())\n",
    "\n",
    "    if year in [2000, 2004]: \n",
    "        print(f'Downscaling for year {year}')\n",
    "        scale_factor= 3\n",
    "    else: scale_factor= 1\n",
    "    \n",
    "    stats = {'Year': year}\n",
    "    for i in range(n_clusters):\n",
    "        remap = numpy.zeros(n_clusters)\n",
    "        remap[i] = 1\n",
    "        class_0 = class_result.remap(list(range(0, n_clusters)), list(remap))\n",
    "        #print(list(remap))\n",
    "        class_stats0 = geemap.image_stats(class_0, scale=30*scale_factor)\n",
    "        #print(class_stats0.getInfo())\n",
    "        \n",
    "        sum_class0 = class_stats0.getInfo()['sum']\n",
    "        sum_clean = int(sum_class0['remapped'])\n",
    "        if PRINT_STATS:\n",
    "            print(legend_keys[i], 'has', sum_clean, 'pixles.')\n",
    "        stats[legend_keys[i]] = sum_clean * (scale_factor**2)\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell only 'defines' a function but does NOT execute it, so we now need to actually run it by calling the function name with appropriate arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map = geemap.Map(center=[22.634, 114.19], zoom=10)\n",
    "\n",
    "# All you need to modify here is the YEAR below: \n",
    "year = 2013\n",
    "all_image, mean, median, ndvi = load_landsat_collection(Map,year,\\\n",
    "                                        shenzhen_buffer, cloud_tolerance = 3,\\\n",
    "                                        DISPLAY_ON_MAP = Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nclusters = 3\n",
    "class_result = unsupervised_classifier(median, shenzhen_buffer, '', n_clusters=nclusters, \n",
    "                                       DISPLAY_ON_MAP = Map)\n",
    "\n",
    "#legend_keys = ['Class1', 'Class2', 'Class3', 'Class4']\n",
    "#legend_keys = ['Class1', 'Class2', 'Class3']\n",
    "legend_keys = [*range(nclusters)]\n",
    "\n",
    "stats = calculate_class_size(class_result, year, legend_keys)\n",
    "print(stats)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add the points taken from the ENVI ROIs to the map\n",
    "for roi in ROIs['ROI']:\n",
    "    visOptions = {'color':roi['color'],'pointRadius':1}\n",
    "    Map.addLayer(roi['Points'],visOptions,roi['name'])\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we executed the `unsupervised_classifier` function, a variable `class_result` was used to received the returned values from the function, and we will use this `class_result` variable in the following code to inspect our classification output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link unsupervised clusters into Land Use Land Cover (LULC) classes\n",
    "You need to carefully compare the K-means results with the original images to decide which cluster belongs to what class. Then, you can name the classes and assign appropriate RGB colours accordingly. However, the classified clusters may differ between different years, so you will need to change and classification setting (i.e. `n_clusters`) and the legend for some of the years. Also, bare in mind, there might be mis-classified pixels. How can you improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_keys = ['Vegetation', 'Urban', 'Agriculture', 'Water']\n",
    "legend_colors = ['#07db00', '#ff525a', '#ffe600', '#2640ff']\n",
    "\n",
    "\"\"\"legend_keys = ['Vegetation', 'Urban', 'Water']\n",
    "legend_colors = ['#07db00', '#ff525a', '#2640ff']\"\"\"\n",
    "\n",
    "\n",
    "Map.addLayer(class_result, {'min': 0, 'max': 3, 'palette': legend_colors}, 'Labelled clusters')\n",
    "Map.add_legend(legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clusters to use depends on the image, and it may different between different years. For the 2019 example, we used 5 clusters and the results seem to include two 'Urban' and two 'Vegetation' clusters. In such situation, we may consider grouping multiple clusters together, by re-mapping the cluster numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reclassify the map. You may want to avoid zero, in case of masking.\n",
    "remapped_class_result = class_result.remap([0, 1, 2, 3, 4], [1, 2, 1, 3, 2])\n",
    "\n",
    "legend_keys = ['Urban', 'Vegetation', 'Water']\n",
    "legend_colors = ['#FFFFB3', '#8DD3C7','#80B1D3']\n",
    "\n",
    "Map.addLayer(remapped_class_result, {'min': 1, 'max': 3, 'palette': legend_colors}, 'Remapped clusters')\n",
    "Map.add_legend(legend_keys=legend_keys, legend_colors=legend_colors, position='bottomright')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, sometimes you may find the reclustered results less satisfactory, so consider carefully before executing the re-mapping code. \n",
    "\n",
    "After remapping the clusters, we have just three final classes now.  Would this be the same as if we ran the `unsupervised_classifier` function with `n_clusters = 3`? Have a try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster areas\n",
    "Next, we need to find out the size of each cluster, i.e. how many pixels belong to each class, and the total size of each class (hint: how big is each pixel?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_size(class_result, year, legend_keys, PRINT_STATS=False):\n",
    "    n_clusters = len(legend_keys)\n",
    "    \n",
    "    #landsat_stats = geemap.image_stats(class_result, scale=90)\n",
    "    #print(landsat_stats.getInfo())\n",
    "\n",
    "    if year in [2000, 2004]: \n",
    "        print(f'Downscaling for year {year}')\n",
    "        scale_factor= 3\n",
    "    else: scale_factor= 1\n",
    "    \n",
    "    stats = {'Year': year}\n",
    "    for i in range(n_clusters):\n",
    "        remap = numpy.zeros(n_clusters)\n",
    "        remap[i] = 1\n",
    "        class_0 = class_result.remap(list(range(0, n_clusters)), list(remap))\n",
    "        #print(list(remap))\n",
    "        class_stats0 = geemap.image_stats(class_0, scale=30*scale_factor)\n",
    "        #print(class_stats0.getInfo())\n",
    "        \n",
    "        sum_class0 = class_stats0.getInfo()['sum']\n",
    "        sum_clean = int(sum_class0['remapped'])\n",
    "        if PRINT_STATS:\n",
    "            print(legend_keys[i], 'has', sum_clean, 'pixles.')\n",
    "        stats[legend_keys[i]] = sum_clean * (scale_factor**2)\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_keys = ['Urban', 'Vegetation', 'Bright Urban',  'Water', 'Shaded Vegetation']        \n",
    "stats_year = calculate_class_size(class_result, year, legend_keys, PRINT_STATS=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the numbers we will need for the R modeling. We will also show you how to export these values automatically later. But for now, have another close examination of the classification results and the functions we used to generated them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat for multiple years\n",
    "By now, you should be able to understand that the `load_landsat_collection` function allows us to just vary the 'year' variable (and leave AOI and Cloud Tolerance constant) to loop through a time series of data. Similarly, for the `unsupervised_classifier` function, we can vary the input 'image' variable (an output from the `load_landsat_collection` function) in order to classify multiple images. Below is a loop built for running K-means classification for Landsat time series for the Shenzhen area. Years between 1987 to 2016 are selected to match our socio-economic data. When this cell is running (you can spot a star key on the LHS of the cell), you can track the progress by looking at the output messages. Once the cell finished running, an integer number will be shown outside of the LHS of the cell, and you can examine the layers in the Map (if DISPLAY_ON_MAP flag was set to True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pixels = []\n",
    "\n",
    "# Only to demonstrate every 10 years, but you should run every year, ideally in groups\n",
    "for year in range(1987, 1990): \n",
    "    \n",
    "    median_image = load_landsat_collection(Map,year, shenzhen_buffer, cloud_tolerance=3,\\\n",
    "                                          MEDIAN_ONLY = True)\n",
    "    class_result = unsupervised_classifier(median_image, shenzhen_buffer, year,\\\n",
    "                    n_clusters=4, DISPLAY_ON_MAP = True) #output_filename=f'Shenzhen_Landsat_Kmeans_{year}.tif'\n",
    "    \n",
    "    ## -- IMPORTANT --  change length of list to match n_clusters\n",
    "    legend_keys = ['Class1', 'Class2', 'Class3', 'Class4'] ## change the class names\n",
    "    \n",
    "    stats = calculate_class_size(class_result, year, legend_keys)\n",
    "    print(stats)\n",
    "    stats_res = numpy.fromiter(stats.values(), dtype=int)\n",
    "    \n",
    "\n",
    "    cluster_pixels.append(stats_res)\n",
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above loop of code to classify a time series, all cluster areas (i.e. pixel counts) have been recorded into the `cluster_pixels` variable, and cluster layers are mapped into the Map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results to excel (.csv) files\n",
    "Lastly, we will use the following code to export the sizes of each class/cluster into a csv file, and you will need this file for R modeling in the next part of the coursework. \n",
    "\n",
    "When using a unsupervised classifier, it is possible that class of interest (Urban, Agricultural for example), will be reordered. If you wish to manually inspect and correct this ordering, the `.csv` file can be opened in excel and editted. Make sure a copy of the original is saved, and the modified version is exported back to a `.csv` file afterwards. Use a simple text editor app to then confirm the original, and modified `.csv` files are of a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results. Do this even if the processing loop crashes.\n",
    "### The results may well be in the memory and can be saved. \n",
    "### When processing in batches, make sure you change the filename so that it doesn't get overwritten for the next batch \n",
    "header = 'Year, ' + ', '.join( legend_keys )\n",
    "numpy.savetxt(\"Shenzhen_pixel_stats_1987_1989.csv\", cluster_pixels, delimiter=\",\", header=header,fmt='%i')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations (on finishing the classification part)!**\n",
    "\n",
    "So far, we have processed the entire time series of Landsat data without actually downloading any of them to our local PC (everything is done on the Cloud!), except the .CSV file. I hope you've enjoyed playing with GEE here. Now you should be able to open the CSV file in excel or R to continue with the [modeling part](https://github.com/qwu-570/GEOG0027_Coursework/blob/2020-2021/docs/6_UrbanModel.ipynb) of the coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP NOW and check you have got all the previous code working and you know what it is doing\n",
    "## ADVANCED TASKS\n",
    "\n",
    "Below you will find some extra code for further advanced classification techniques. These are harder to use and will require some concentration and consideration of what the code is asking GEE to do. The following tasks are not strictly neccessary for the coursework, but are satisfying, and once complete, may allow for easier analysis and use within the modelling section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ENVI ROIs for ground truthing\n",
    "If you have previously saved a collection of ROIS as a .csv file in ENVI, we can use that file to create training data for GEE.\n",
    "GEE likes to get training data as a series of points. These can be extracted from the ENVI file using a module to be downloaded. Use a terminal in jupyterhub to donwload the module using the following command and make sure it is in the same directory as this notebook.\n",
    "\n",
    "`wget https://github.com/UCL-EO/GEOG0027_Coursework/blob/main/docs/ENVI_ROI.py`\n",
    "\n",
    "We will use a function that will load information we need from a saved collection of ROIs created in ENVI. To call it you need to tell it where to find the .csv file with the ROIs in it. There is an example file given, with 3 classes set from 1986. You will need your own file for the coursework. Download the example file with:\n",
    "\n",
    "\n",
    "`wget https://github.com/UCL-EO/GEOG0027_Coursework/blob/main/docs/1986_3type_ROIs.csv`\n",
    "\n",
    "To make a successful classification, make sure the ROIs are valid for all time points you wish to classify. As the Shenzhen region undergoes many changes since 1984, you may wish to create seperate ROI files for the full time series. This decision is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ENVI_ROI\n",
    "from ENVI_ROI import get_ENVI_ROIS\n",
    "importlib.reload(ENVI_ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ENVI_ROI import get_ENVI_ROIS\n",
    "roi_file = 'gt_ROI_n3.csv' \n",
    "# roi_file = '1986_3type_ROIs.csv' ### original file, it works, but not very well\n",
    "ROIs = get_ENVI_ROIS(roi_file,points_per_region=100) \n",
    "### points_per_region is the number of subsamples to use to send to GEE. \n",
    "### More points send more training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ENVI_ROI import get_ENVI_ROIS\n",
    "\n",
    "# ground truth ROIs (n=4)\n",
    "roi_file = 'gt_ROI_n4.csv'\n",
    "ROIs = get_ENVI_ROIS(roi_file,points_per_region=100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will plot the points from the ROI file to the map. \n",
    "> Are the points in the location you expect them to be in?\n",
    ">\n",
    "> Do the ROI regions created for an image from 1986 align well with 2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add the points taken from the ENVI ROIs to the map\n",
    "for roi in ROIs['ROI']:\n",
    "    visOptions = {'color':roi['color'],'pointRadius':1}\n",
    "    Map.addLayer(roi['Points'],visOptions,roi['name'])\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classification accuracy (ADVANCED)\n",
    "Another task we can perform using the ROIs from ENVI is to check whether a classification is accurate.\n",
    "This is a valid technique for the previous unsupervised classification and the code is set up to access these results (variable `class_result`). \n",
    "Why does it not tell us much about the supervised classification using the code below? How can you independently asses this classification? (Hint: you'll need to copy some code cells, and have another file from ENVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(class_result, ROIs):\n",
    "    '''This function provides a simple error matrix using\n",
    "    the ROIS loaded previously from an ENVI file.\n",
    "    '''\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    #print(trainingFeatures.getInfo())\n",
    "    class_points= class_result.reduceRegions(collection=trainingFeatures,reducer=ee.Reducer.median(),\n",
    "                                             scale= 500,crs= 'EPSG:5070')\n",
    "    # print(class_points.getInfo())\n",
    "    return class_points.errorMatrix('class','median').getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pixel error matrix for the unsupervied classification\n",
    "matrix = compute_errors(class_result,ROIs)\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the error matrix to the time series\n",
    "\n",
    "For the time series processing above, you may wish to save the error matrix within the `'csv` file. This will enable a more informed manual adjusting of the pixel count results. The code below will need to be added to the loop above in the correct place to add the error matrix to the pixel stats file (remember to indent it for the loop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------ CODE BLOCK ----- START\n",
    "error_matrix = compute_errors(class_result,ROIs)\n",
    "stats_res = numpy.hstack([stats_res,numpy.array(error_matrix).flatten()])\n",
    "legend_keys = legend_keys+['EM_{}{}'.format(m,n) for m in range(len(error_matrix)) \n",
    "                                             for n in range(len(error_matrix))]\n",
    "### ------ CODE BLOCK ----- END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_pixels = []\n",
    "\n",
    "# Only to demonstrate every 10 years, but you should run every year 1987-2016, ideally in groups\n",
    "for year in range(1987, 2017): \n",
    "    \n",
    "    median_image = load_landsat_collection(Map, year, shenzhen_buffer, cloud_tolerance=3,\\\n",
    "                                          MEDIAN_ONLY = True)\n",
    "    class_result = unsupervised_classifier(median_image, shenzhen_buffer, year,\\\n",
    "                    n_clusters=3, DISPLAY_ON_MAP = True) #output_filename=f'Shenzhen_Landsat_Kmeans_{year}.tif'\n",
    "    \n",
    "    ## -- IMPORTANT --  change length of list to match n_clusters\n",
    "    legend_keys = ['Class1', 'Class2', 'Class3'] ## change the class names\n",
    "    \n",
    "    stats = calculate_class_size(class_result, year, legend_keys)\n",
    "    print(stats)\n",
    "    stats_res = numpy.fromiter(stats.values(), dtype=int)\n",
    "    \n",
    "    error_matrix = compute_errors(class_result,ROIs)\n",
    "    stats_res = numpy.hstack([stats_res,numpy.array(error_matrix).flatten()])\n",
    "    legend_keys = legend_keys+['EM_{}{}'.format(m,n) for m in range(len(error_matrix)) \n",
    "                                             for n in range(len(error_matrix))]\n",
    "    \n",
    "    cluster_pixels.append(stats_res)\n",
    "#Map\n",
    "\n",
    "# header = 'Year, ' + ', '.join( legend_keys )\n",
    "# numpy.savetxt(\"Shenzhen_pixel_stats_em_2001_2016.csv\", cluster_pixels, delimiter=\",\", header=header,fmt='%i')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results. Do this even if the processing loop crashes.\n",
    "### The results may well be in the memory and can be saved. \n",
    "### When processing in batches, make sure you change the filename so that it doesn't get overwritten for the next batch \n",
    "header = 'Year, ' + ', '.join( legend_keys )\n",
    "numpy.savetxt(\"Shenzhen_pixel_stats_em_n3_1987_2016_v2.csv\", cluster_pixels, delimiter=\",\", header=header,fmt='%i')       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a supervised classification (ADVANCED)\n",
    "Another task we can perform using the ROIs from ENVI is to use them for a supervised classification. The GEE library includes a Minimum Distance Algorithm that we can use.\n",
    "\n",
    "The following function will perform the supervised Minimum Distance classification algorithm using the points taken from the ROI file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_distance(image, ROIs, year, output_filename='', DISPLAY_ON_MAP = False):\n",
    "    '''This function provides a simple minimum distance classifier,\n",
    "    using the ROIS loaded previously from an ENVI file.\n",
    "    Optional arguments:\n",
    "    output_filename should be a quoted string, e.g. 'Shenzhen_Landsat_Kmeans_2019.tif';\n",
    "    DISPLAY_ON_MAP can be switched on, so the cluster map will be added to Map.\n",
    "    '''\n",
    "    \n",
    "    # Make the training dataset:\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    classifierTraining = image.sampleRegions(collection= trainingFeatures,\n",
    "            properties= ['class'],\n",
    "            scale= 30\n",
    "        )\n",
    "    \n",
    "    # Instantiate the clusterer and train it.\n",
    "    bands=['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n",
    "    # Option for independent work here: can you change the 'minimumDistance' to another algorithm?\n",
    "    # You'll need to get internet searching for the GEE options to try\n",
    "    classifier = ee.Classifier.minimumDistance().train(\n",
    "        features= classifierTraining,\n",
    "        classProperty= 'class',\n",
    "        inputProperties= bands\n",
    "    )\n",
    "\n",
    "    # Cluster the input using the trained clusterer.\n",
    "    class_result = image.classify(classifier)\n",
    "    \n",
    "    if type(DISPLAY_ON_MAP) == geemap.geemap.Map:\n",
    "        # Display the clusters with ROI set colors.\n",
    "        Map.addLayer(class_result, {'min': 0, 'max': ROIs['n_ROI']-1, 'palette': [roi['color'] for roi in ROIs['ROI']]},\n",
    "                    'MinDist '+str(year))\n",
    "\n",
    "    if output_filename == '':\n",
    "        print(f'{year} classification finished. No output exported.')\n",
    "    else:\n",
    "        #Export the result directly to your computer/Hub:\n",
    "        geemap.ee_export_image(class_result, filename=output_filename, \\\n",
    "                            scale=900, region=aoi, file_per_band=False) \n",
    "        # When scale is small, GEE won't allow downloading due to size limitation\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(image, ROIs, year, output_filename='', DISPLAY_ON_MAP = False):\n",
    "    \n",
    "    # Make the training dataset:\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    classifierTraining = image.sampleRegions(collection= trainingFeatures,\n",
    "            properties= ['class'],\n",
    "            scale= 30\n",
    "        )\n",
    "    \n",
    "    # Instantiate the clusterer and train it.\n",
    "    bands=['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n",
    "    # Option for independent work here: can you change the 'minimumDistance' to another algorithm?\n",
    "    # You'll need to get internet searching for the GEE options to try\n",
    "    classifier = ee.Classifier.smileRandomForest(10).train(\n",
    "        features= classifierTraining,\n",
    "        classProperty= 'class',\n",
    "        inputProperties= bands\n",
    "    )\n",
    "\n",
    "    # Cluster the input using the trained clusterer.\n",
    "    class_result = image.classify(classifier)\n",
    "    \n",
    "    if type(DISPLAY_ON_MAP) == geemap.geemap.Map:\n",
    "        # Display the clusters with ROI set colors.\n",
    "        Map.addLayer(class_result, {'min': 0, 'max': ROIs['n_ROI']-1, 'palette': [roi['color'] for roi in ROIs['ROI']]},\n",
    "                    'RandForest '+str(year))\n",
    "\n",
    "    if output_filename == '':\n",
    "        print(f'{year} classification finished. No output exported.')\n",
    "    else:\n",
    "        #Export the result directly to your computer/Hub:\n",
    "        geemap.ee_export_image(class_result, filename=output_filename, \\\n",
    "                            scale=900, region=aoi, file_per_band=False) \n",
    "        # When scale is small, GEE won't allow downloading due to size limitation\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(image, ROIs, year, output_filename='', DISPLAY_ON_MAP = False):\n",
    "    \n",
    "    # Make the training dataset:\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    classifierTraining = image.sampleRegions(collection= trainingFeatures,\n",
    "            properties= ['class'],\n",
    "            scale= 30\n",
    "        )\n",
    "    \n",
    "    # Instantiate the clusterer and train it.\n",
    "    bands=['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']\n",
    "    # Option for independent work here: can you change the 'minimumDistance' to another algorithm?\n",
    "    # You'll need to get internet searching for the GEE options to try\n",
    "    classifier = ee.Classifier.libsvm().train(\n",
    "        features= classifierTraining,\n",
    "        classProperty= 'class',\n",
    "        inputProperties= bands\n",
    "    )\n",
    "\n",
    "    # Cluster the input using the trained clusterer.\n",
    "    class_result = image.classify(classifier)\n",
    "    \n",
    "    if type(DISPLAY_ON_MAP) == geemap.geemap.Map:\n",
    "        # Display the clusters with ROI set colors.\n",
    "        Map.addLayer(class_result, {'min': 0, 'max': ROIs['n_ROI']-1, 'palette': [roi['color'] for roi in ROIs['ROI']]},\n",
    "                    'SVM '+str(year))\n",
    "\n",
    "    if output_filename == '':\n",
    "        print(f'{year} classification finished. No output exported.')\n",
    "    else:\n",
    "        #Export the result directly to your computer/Hub:\n",
    "        geemap.ee_export_image(class_result, filename=output_filename, \\\n",
    "                            scale=900, region=aoi, file_per_band=False) \n",
    "        # When scale is small, GEE won't allow downloading due to size limitation\n",
    "\n",
    "    return class_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we call the classifier and view it on the map. \n",
    "Check the layers to see if the classifier has matched the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4 ROIs\n",
      "water total 6 subregions\n",
      "vegetation total 12 subregions\n",
      "urban total 12 subregions\n",
      "agriculture total 2 subregions\n",
      "Successfully loaded 4 ROIs\n",
      "gt_water total 5 subregions\n",
      "gt_vegetation total 4 subregions\n",
      "gt_urban total 3 subregions\n",
      "gt_agriculture total 2 subregions\n"
     ]
    }
   ],
   "source": [
    "from ENVI_ROI import get_ENVI_ROIS \n",
    "\n",
    "# ROIs for supervised \n",
    "roi_file_supervised = 'supervised_ROIs_final.csv'\n",
    "ROIs_supervised = get_ENVI_ROIS(roi_file_supervised,points_per_region=100) \n",
    "\n",
    "# ground truth ROIs (n=4)\n",
    "roi_file = 'gt_ROIs_final.csv'\n",
    "ROIs = get_ENVI_ROIS(roi_file,points_per_region=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### add the points taken from the ENVI ROIs to the map\n",
    "for roi in ROIs['ROI']:\n",
    "    visOptions = {'color':roi['color'],'pointRadius':1}\n",
    "    Map.addLayer(roi['Points'],visOptions,roi['name'])\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map = geemap.Map(center=[22.634, 114.19], zoom=10)\n",
    "\n",
    "# All you need to modify here is the YEAR below:\n",
    "year = 2014\n",
    "median = load_landsat_collection(Map,year,\\\n",
    "                                 shenzhen_buffer, cloud_tolerance = 3,\\\n",
    "                                 DISPLAY_ON_MAP = Map, MEDIAN_ONLY = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " classification finished. No output exported.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b61eed70b974ed886835338ef31d70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(bottom=57372.0, center=[22.634, 114.19], controls=(WidgetControl(options=['position', 'transparent_bg'], w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Complete the supervised classification for the given year\n",
    "mindist = minimum_distance(median,ROIs_supervised,\"\",DISPLAY_ON_MAP=Map)\n",
    "#randforest = random_forest(median,ROIs_supervised,\"\",DISPLAY_ON_MAP=Map)\n",
    "#svm_results = svm(median,ROIs_supervised,\"\",DISPLAY_ON_MAP=Map)\n",
    "\n",
    "Map.add_legend(legend_keys=[roi['name'] for roi in ROIs_supervised['ROI']], \n",
    "               legend_colors=[roi['color'] for roi in ROIs_supervised['ROI']], \n",
    "               position='bottomright')\n",
    "\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classification accuracy \n",
    "Again we can test the classification accuracy. However, irn it's current from, why does this code example not truely asses the accuracy of the classification?\n",
    ">How can you independently asses this classification? \n",
    ">\n",
    "> (Hint: you'll need to copy some code cells, and have another file from ENVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ENVI_ROI import get_ENVI_ROIS \n",
    "\n",
    "# ROIs for supervised \n",
    "roi_file_supervised = 'supervised_ROI_n4_V1.csv'\n",
    "ROIs_supervised = get_ENVI_ROIS(roi_file_supervised,points_per_region=100) \n",
    "\n",
    "# ground truth ROIs (n=4)\n",
    "roi_file = 'gt_ROI_n4.csv'\n",
    "ROIs = get_ENVI_ROIS(roi_file,points_per_region=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(class_result, ROIs):\n",
    "    '''This function provides a simple error matrix using\n",
    "    the ROIS loaded previously from an ENVI file.\n",
    "    '''\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    #print(trainingFeatures.getInfo())\n",
    "    class_points= class_result.reduceRegions(collection=trainingFeatures,reducer=ee.Reducer.median(),\n",
    "                                             scale= 500,crs= 'EPSG:5070')\n",
    "    # print(class_points.getInfo())\n",
    "    return class_points.errorMatrix('class','median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### pixel error matrix for the supervised classification\n",
    "mindist_em = compute_errors(mindist,ROIs)\n",
    "randforest_em = compute_errors(randforest,ROIs)\n",
    "svm_em = compute_errors(svm_results,ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"minimum distance\")\n",
    "print(mindist_em.getInfo())\n",
    "print(\"kappa: \" + str(mindist_em.kappa().getInfo()))\n",
    "print(\"accuracy: \" + str(mindist_em.accuracy().getInfo()))\n",
    "print(\"consumer: \" + str(mindist_em.consumersAccuracy().getInfo()))\n",
    "print(\"producer: \" + str(mindist_em.producersAccuracy().getInfo()))\n",
    "print(\"random forest\")\n",
    "print(randforest_em.getInfo())\n",
    "print(\"kappa: \" + str(randforest_em.kappa().getInfo()))\n",
    "print(\"accuracy: \" + str(randforest_em.accuracy().getInfo()))\n",
    "print(\"consumer: \" + str(randforest_em.consumersAccuracy().getInfo()))\n",
    "print(\"producer: \" + str(randforest_em.producersAccuracy().getInfo()))\n",
    "print(\"svm\")\n",
    "print(svm_em.getInfo())\n",
    "print(\"kappa: \" + str(svm_em.kappa().getInfo()))\n",
    "print(\"accuracy: \" + str(svm_em.accuracy().getInfo()))\n",
    "print(\"consumer: \" + str(svm_em.consumersAccuracy().getInfo()))\n",
    "print(\"producer: \" + str(svm_em.producersAccuracy().getInfo()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series supervised classification (ADVANCED)\n",
    "Here is an example loop that will perform a supervised classifcation for multiple years. If you want to use this method, then some work will be required.\n",
    "\n",
    "Can you make sure the supervised classification is performing correctly for each step of the loop?\n",
    "Can you compute the error matrix (independently) for each step of the loop?\n",
    "\n",
    "Can you modify the unsupervised classification loop to add the error matrix entries too?\n",
    "Saving the error matrix for the unsupervised classifier will allow for manual editting of the saved results .csv file. This is sometimes required prior to modelling so that the columns match from year to year (clusters can switch order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth ROIs for supervised (n=4)\n",
    "roi_file = 'gt_ROI_n4.csv'\n",
    "ROIs = get_ENVI_ROIS(roi_file,points_per_region=100) \n",
    "\n",
    "# ROIs for supervised \n",
    "roi_file_supervised = 'supervised_ROI_n4_V1.csv'\n",
    "ROIs_supervised = get_ENVI_ROIS(roi_file_supervised,points_per_region=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(class_result, ROIs):\n",
    "    '''This function provides a simple error matrix using\n",
    "    the ROIS loaded previously from an ENVI file.\n",
    "    '''\n",
    "    trainingFeatures = ee.FeatureCollection([roi['Points'] for roi in ROIs['ROI']]).flatten()\n",
    "    #print(trainingFeatures.getInfo())\n",
    "    class_points= class_result.reduceRegions(collection=trainingFeatures,reducer=ee.Reducer.median(),\n",
    "                                             scale= 500,crs= 'EPSG:5070')\n",
    "    # print(class_points.getInfo())\n",
    "    return class_points.errorMatrix('class','median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991 classification finished. No output exported.\n",
      "{'Year': 1991, 'water': 901279, 'vegetation': 1552827, 'urban': 995699, 'agriculture': 309116}\n",
      "[[323, 12, 0, 165], [0, 359, 9, 32], [0, 0, 300, 0], [0, 33, 2, 165]]\n",
      "1992 classification finished. No output exported.\n",
      "{'Year': 1992, 'water': 889935, 'vegetation': 1552096, 'urban': 983956, 'agriculture': 332933}\n",
      "[[230, 6, 0, 264], [0, 372, 6, 22], [0, 1, 299, 0], [0, 15, 0, 185]]\n"
     ]
    }
   ],
   "source": [
    "cluster_pixels = []\n",
    "\n",
    "# Only to demonstrate every 10 years, but you should run every year, ideally in groups\n",
    "for year in range(1991,1993): \n",
    "    \n",
    "    median = load_landsat_collection(Map, year, shenzhen_buffer, cloud_tolerance=3,\n",
    "                                     MEDIAN_ONLY = True)\n",
    "\n",
    "    class_result = minimum_distance(median,ROIs_supervised,year,DISPLAY_ON_MAP=False)\n",
    "    \n",
    "    legend_keys = [roi['name'] for roi in ROIs_supervised['ROI']]\n",
    "    \n",
    "    stats = calculate_class_size(class_result, year, legend_keys)\n",
    "    print(stats)\n",
    "    stats_res = numpy.fromiter(stats.values(), dtype=int)\n",
    "    ### these next lines will also compute an error matrix for the results\n",
    "    ### To make them work, you'll need to supply an alternative ROIs from an ENVI file \n",
    "    ### These lines can also be used to add error matrix information to the unsupervised \n",
    "    ### classification loop above too\n",
    "    ### ------ CODE BLOCK ----- START\n",
    "    error_matrix = compute_errors(class_result,ROIs)\n",
    "    print(error_matrix.getInfo())\n",
    "    stats_res = numpy.hstack([stats_res,numpy.array(error_matrix.getInfo()).flatten()])\n",
    "    legend_keys = legend_keys+['EM_{}{}'.format(m,n) for m in range(len(error_matrix.getInfo())) \n",
    "                                                      for n in range(len(error_matrix.getInfo()))]\n",
    "    ### ------ CODE BLOCK ----- END\n",
    "    \n",
    "    kappa = error_matrix.kappa().getInfo()\n",
    "    accuracy = error_matrix.accuracy().getInfo()\n",
    "    stats_res = numpy.hstack([stats_res,[kappa,accuracy]])\n",
    "    legend_keys = legend_keys+['kappa', 'accuracy']\n",
    "    \n",
    "    producers = error_matrix.producersAccuracy().getInfo()\n",
    "    stats_res = numpy.hstack([stats_res,numpy.array(producers).flatten()])\n",
    "    legend_keys = legend_keys+['P1', 'P2', 'P3', 'P4']\n",
    "    \n",
    "    consumers = error_matrix.consumersAccuracy().getInfo()\n",
    "    stats_res = numpy.hstack([stats_res,numpy.array(consumers).flatten()])\n",
    "    legend_keys = legend_keys+['C1', 'C2', 'C3', 'C4']\n",
    "\n",
    "    cluster_pixels.append(stats_res)\n",
    "#Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the results. Do this even if the processing loop crashes.\n",
    "### The results may well be in the memory and can be saved. \n",
    "### When processing in batches, make sure you change the filename so that it doesn't get overwrtten for the next batch \n",
    "header = 'Year, ' + ', '.join( legend_keys )\n",
    "fmt = '%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%i','%1.5f','%1.5f','%1.3f','%1.3f','%1.3f','%1.3f','%1.3f','%1.3f','%1.3f','%1.3f' \n",
    "numpy.savetxt(\"MD_Shenzhen_pixel_stats_9192.csv\", cluster_pixels, delimiter=\",\", header=header,fmt=fmt)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:geog0027-geemap]",
   "language": "python",
   "name": "conda-env-geog0027-geemap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
